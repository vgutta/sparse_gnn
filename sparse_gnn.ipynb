{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d45ddb8-2a5b-4542-bae2-115f30064f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b149a7e6-1c19-4909-94c8-c98a108aef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4d1e7f-6958-4eda-9236-ff64ba8ba845",
   "metadata": {},
   "source": [
    "### Transform to generate train, val, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "043b6a10-4be6-413c-ab96-9ef62f372aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = RandomNodeSplit('train_rest', key='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca7ec12-d2ab-4ecc-af12-c67a8aa7bfe9",
   "metadata": {},
   "source": [
    "### Matrix Market Graph (bcspwr07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf3a2fe6-58ba-43b7-971d-f776fed4acb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182028/1057088530.py:2: DeprecationWarning: \n",
      "\n",
      "The scipy.sparse array containers will be used instead of matrices\n",
      "in Networkx 3.0. Use `from_scipy_sparse_array` instead.\n",
      "  mmg2 = nx.from_scipy_sparse_matrix(bcspwr07)\n"
     ]
    }
   ],
   "source": [
    "bcspwr07 = sp.io.mmread(\"./matrix_market_graphs/bcspwr07.mtx\")\n",
    "mmg2 = nx.from_scipy_sparse_matrix(bcspwr07)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c0d16-b030-47df-9752-d7246f4476b6",
   "metadata": {},
   "source": [
    "#### convert scipy coo_matrix format to Torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f8c55e-20ba-4554-818e-b051a21947f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = bcspwr07.data\n",
    "indices = np.vstack((bcspwr07.row, bcspwr07.col))\n",
    "i = torch.LongTensor(indices)\n",
    "v = torch.FloatTensor(values)\n",
    "shape = bcspwr07.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38c03ddb-57ba-477d-b411-93e36340f8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(crow_indices=tensor([   0,    3,    5,  ..., 5817, 5821, 5824]),\n",
      "       col_indices=tensor([   0,   32,  143,  ..., 1595, 1610, 1611]),\n",
      "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]), size=(1612, 1612),\n",
      "       nnz=5824, layout=torch.sparse_csr)\n"
     ]
    }
   ],
   "source": [
    "sparse_tensor = torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "print(sparse_tensor.to_sparse_csr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267c78d-bd00-4bf1-970c-895a533cb0e2",
   "metadata": {},
   "source": [
    "#### create data object from Networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc48182c-6be8-46d8-a438-eadc524d5afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = from_networkx(mmg2)\n",
    "data.x = sparse_tensor.to_sparse_csr()\n",
    "data.y = torch.randint(0,10, [data.num_nodes,])  # random y values since this dataset is not curated\n",
    "data = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfc6171e-9046-4044-a99f-aa6f8ae46fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.num_features = 1612\n",
    "data.num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5648057b-a869-4f36-90de-88d7332d3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(data.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, data.num_classes)\n",
    "        \n",
    "    def forward(self, data):\n",
    "         x, edge_index = data.x, data.edge_index\n",
    "         x = self.conv1(x, edge_index)\n",
    "         x = F.relu(x)\n",
    "         x = F.dropout(x, training=self.training)\n",
    "         x = self.conv2(x, edge_index)\n",
    "         return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a883f-3fb4-4909-a34b-f9fcf3e6a1c8",
   "metadata": {},
   "source": [
    "### CUDA errors for sparse_csr data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "598fe30b-d96f-47b9-ab06-f37233182b01",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::empty_strided' with arguments from the 'SparseCsrCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty_strided' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterCUDA.cpp:26493 [kernel]\nMeta: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterBackendSelect.cpp:665 [kernel]\nPython: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradMLC: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/TraceType_2.cpp:11423 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#device = torch.device('cpu')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Net()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gnnproxy/lib/python3.9/site-packages/torch_geometric/data/data.py:216\u001b[0m, in \u001b[0;36mBaseData.to\u001b[0;34m(self, device, non_blocking, *args)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m*\u001b[39margs: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    213\u001b[0m        non_blocking: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    only the ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gnnproxy/lib/python3.9/site-packages/torch_geometric/data/data.py:199\u001b[0m, in \u001b[0;36mBaseData.apply\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03mthe ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstores:\n\u001b[0;32m--> 199\u001b[0m     \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/gnnproxy/lib/python3.9/site-packages/torch_geometric/data/storage.py:148\u001b[0m, in \u001b[0;36mBaseStorage.apply\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03mthe ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m \u001b[43mrecursive_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/gnnproxy/lib/python3.9/site-packages/torch_geometric/data/storage.py:498\u001b[0m, in \u001b[0;36mrecursive_apply\u001b[0;34m(data, func)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursive_apply\u001b[39m(data: Any, func: Callable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor):\n\u001b[0;32m--> 498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mPackedSequence):\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(data)\n",
      "File \u001b[0;32m~/.conda/envs/gnnproxy/lib/python3.9/site-packages/torch_geometric/data/data.py:217\u001b[0m, in \u001b[0;36mBaseData.to.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m*\u001b[39margs: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    213\u001b[0m        non_blocking: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    only the ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::empty_strided' with arguments from the 'SparseCsrCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty_strided' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterCUDA.cpp:26493 [kernel]\nMeta: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterBackendSelect.cpp:665 [kernel]\nPython: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradMLC: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/TraceType_2.cpp:11423 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "model = Net().to(device)\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a920077-6b4d-45b9-971f-d51fe249043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "model = Net().to(device)\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23d9c08e-f9e2-4b91-a47c-32af8f101cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "     optimizer.zero_grad()\n",
    "     out = model(data)\n",
    "     loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "     #loss.backward()\n",
    "     optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0105f6c8-ffe1-4e44-a1f9-7f12f5d63240",
   "metadata": {},
   "source": [
    "### Backprop causes an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bfc0dea-a5b5-4323-b57d-2dd15832b254",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::empty.memory_format' with arguments from the 'SparseCsrCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty.memory_format' is only available for these backends: [CPU, CUDA, Meta, MkldnnCPU, SparseCPU, SparseCUDA, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterCUDA.cpp:26493 [kernel]\nMeta: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCUDA: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterSparseCUDA.cpp:1060 [kernel]\nBackendSelect: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterBackendSelect.cpp:665 [kernel]\nPython: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradMLC: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/TraceType_2.cpp:11423 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[0;32m----> 7\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.conda/envs/gnnproxy/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gnnproxy/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::empty.memory_format' with arguments from the 'SparseCsrCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty.memory_format' is only available for these backends: [CPU, CUDA, Meta, MkldnnCPU, SparseCPU, SparseCUDA, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterCUDA.cpp:26493 [kernel]\nMeta: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCUDA: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterSparseCUDA.cpp:1060 [kernel]\nBackendSelect: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/build/aten/src/ATen/RegisterBackendSelect.cpp:665 [kernel]\nPython: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradMLC: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/VariableType_2.cpp:10483 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/autograd/generated/TraceType_2.cpp:11423 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1640811803361/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "     optimizer.zero_grad()\n",
    "     out = model(data)\n",
    "     loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "     loss.backward()\n",
    "     optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa7c28-73e4-4841-90e3-af8fbf7532de",
   "metadata": {},
   "source": [
    "### Low accuracy due to randomly generated y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fcdedc3-83b7-4ff7-8478-4ae8649f6563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1020\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / int(data.test_mask.sum())\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367a4fe-0b2f-4af2-b0d0-dcc3c777f9db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnproxy",
   "language": "python",
   "name": "gnnproxy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
